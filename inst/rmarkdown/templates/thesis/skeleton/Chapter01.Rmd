---
title: "Introduction"
author: "Cesar Miguel Valdez Cordova"
date: "December 15, 2018"
output: html_document
---

# Biological Background

## Cells contain active and latent information 

All living organisms currently documented have cells as their fundamental unit.  Cells preserve all information essential for reproduction, functioning and survival in the form of DNA. Said information is dynamically and specifically deployed through the process of gene expression. This process is greatly influenced by Transcription Factors (TF), which are sequence specific DNA-binding proteins that ultimately have an effect over the level of expression of per-organism given DNA sequences called genes [@Benveniste_2014]. Chromatin is the state in which DNA is packaged within the cell; it's core structure, the nucleosome, is an eight-protein complex that consists of two equal pairs of four different histones, which are wrapped with 147 base pairs of DNA [@Kouzarides_2007]. The accessibility and function of chromatin features change when covalent modifications are present on histones and DNA strands. The effects of these modifications are often position and compound dependant; gene expression is impacted in specific, often recurrent patterns given minute, non uniformly distributed, changes in particular histones across different genomic regions [@Zhang_2017]. Generally speaking, chromatin state modifying elements can regulate gene expression by repression or activation, states which are mutually exclusive [@Charlet_2016][@Tie_2009].  In addition, chromatin modification patterns can provide insights to elucidate active or repressed key regulators in a given cell across a range of conditions [@Creyghton_2010]. Annotating said regulatory elements and identifying related factors are key steps in understanding how cells function and similarly, how they cease to do so when afflicted with a variety of cell state stability perturbing conditions [@Slattery_2014]. A schematic containing a few of these elements and their relative positions in the genome can be visualized in Figure 1.

![Relevant biological constructs and their relative positions in the genome. Obtained from ^[https://haemgen.haem.cam.ac.uk/genomebrowser/encode/]](./Figures/encodenaturegraphic.png)

## Binding affinity information can be experimentally obtained

Transcription Factor binding affinity is intrinsically variable in magnitude
and specificity. Due to the great diversity in chromatin states and histone
modification patterns surrounding binding sites, putative sites in vivo can be
highly inconsistent across cell types [@Yang_2017]. Diverse high-throughput
technologies have been developed to specifically study said phenomenon *in
vivo*. Protein binding experiments based on microarrays coupled with
high-throughput sequencing have been successful at quantifying sequence and
related domain specific affinities [@Slattery_2011]. These experiments are
generally represented as Position Weight Matrices (PWM), where each matrix
element represents a contribution to overall binding affinity, which can later
be incorporated into comprehensive affinity models [@Orenstein_2014]. While
PWMs generally give satisfactory approximations of transcription factor binding
with simple specificity models, said methods fail where the *in vivo* TF
binding mechanism is complex. Such is the case where multiple mutations occur
or multiple affinity predictors are involved [@Zhao_2011]. When dealing with
specific biological processes, Chromatin Immunoprecipitation followed by
sequencing (ChIP-Seq) is the most direct way to identify modified histone
locations and related DNA binding proteins [@Furey_2012]. Experimental designs
utilizing said technique can be intricately tailored to extract
phenomenon-specific biologically relevant information that can be subsequently
modeled. Briefly, ChIP-Seq allows to determine where a protein, including but
not limited to transcription factors, bind to within the genome. This is achieved by
chemically cross-linking proteins and chromatin, isolating said
protein-chromatin pairs, capturing the DNA fragments bound to proteins using
corresponding specific antibodies, and finally, sequencing the sample with Next
Generation Sequencing technologies [@Bailey_2013]. Once the NGS experiment is
successfully executed, consequent analysis can be divided into three steps:
Filteing out reads that fall below some quality threshold, aligning the reads
to a reference genome or transcriptome and quantifying gene expression levels, or in the case of ChIP-Seq experiments, peak calling [@Fonseca_2014]. A visual representation of a typical ChIP and related experiments may be visualized in Figure 2.

![ChIP and associated experimental protocols utilized to extract relevant binding site information. Obtained from [@Furey_2012].](./Figures/exprotocol.png)

## Biological experiments produce noisy data 
 
Gene expression is a stochastic process. Therefore, biological variability will
be present when measuring between samples that are considered to be a part of
the same population [@Hansen_2011]. Additionally, protocols for  ChIP-Seq
preparation involve several steps, all of which are prone to the introduction
of experimental bias. Biological phenomena related  sources for noise can
include but are not limited to: Chromatin structure, enzymatic cleavage,
nucleic acid isolation, PCR amplification, and Read mapping effects
[@Meyer_2014]. In detail, bias may be observed when one transcription factor
binds to alternative binding sites, which will show the same signal at multiple
positions in the same genome and introduce bias [@Li_2010]. Similarly, Genomic
DNA may not be accessible in certain conformations given a chosen protocol,
causing experimental technique specific bias to not be fully accounted for when
developing consequent predictive models [@Xin_2018]. ChIP-dependant bias has
also been shown to increase if the crosslinking  duration is extended
[@Baranello_2015]. As is the case with the choice of antibody for the
immunoprecipitation step, which directly impacts the quality of ChIP seq data
obtained [@Busby_2016]. These potential sources of error must be considered
before executing protocols and can be optimized according to the biological
question trying to be answered for increasingly accurate inferential results. 

## Technique normalization 

Technique-specific sources of bias can be identified and systematically
corrected in Next-Generation Sequencing (NGS) protocols. Specifically, through
characterizing bias, adjusting for sequncing depth, handling duplicate reads
and consequent modeling variations [@Sims_2014]. Sequencing depth refers to the
average number of times a specific nucleotide has been sequenced, given an NGS
experiment. This parameter directly influences the accuracy by which events can
be quantified in Seq based assays. Choosing an appropriate normalization scheme
requires prior knowledge of the system. Count method based normalization
metrics usually include: Total Count, Upper quartile, Mean of M, Median and
Reads Per Kilobase per Million mapped reads [@Dillies_2012]. When the libraries
being normalized have different read distributions, it is advised to implement
scaling methods which consist on identifying the background signal component
and scaling the input to match said signal. Generally, data must be separated
into a signal component, corresponding to binding regions, and a noise
component. Binning methods have been developed to discern between these
signals. Most methods use either a pre-determined cut-off or threshold
estimation directly from data as separation criteria [@Aleksic_2014]. Other
methods treat both signals as a linear combination, from where expected noise
can be estimated [@Kharchenko_2008] [@Xu_2010] [@Diaz_2012].

## Enrichment bias and the differential binding problem

Once experimental artefacts are accounted for, bias must be quantified within
and in the case of multi-sample experiments, between enriched regions.
Quantitative comparison of ChIP-seq data is often referred to as the
differential binding problem. Commonly used methods for cross sample comparison
of ChIP-seq data include MAnorm. Specifically developed for ChIP-seq data, it
scales the reads of peaks common to two samples using a robust linear
regression based on the MA plot: Logarithmic ratio of reads in the two samples
relative to mean log read counts [@Shao_2012]. In a similar fashion, ChIPcomp
performs quantitative comparison of multiple ChIP samples, which measures
genomic background using control data and considers multiple-factor
experimental designs. Signals are estimated from peaks detected from the union
of samples, which are assumed to follow a poisson distribution [@Chen_2015].

ChIP-Seq data generally exhibits high signal-to-noise ratio, although
between-sample variability remains a challenge.  The ENCODE consortium has
proposed two metrics to assess the degree of noise in ChIP-seq samples:
Fraction of reads in peaks and cross correlation profiles [@Landt_2012]. Only
the former requires explicit peak calling for evaluation, which introduces a
degree of subjectivity in the metric. The latter assesses read clustering
levels. However, having a high signal to noise ratio doesn't guarantee genuine
binding sites; it merely indicates the presence of a genome with multiple
read-enriched regions or non-specific binding sites [@Nakato_2016]. In fact,
most DNA-Binding proteins, transcription factors included, show a degree of non
specificity; in these cases, quantifying binding affinity is a potential
solution [@Gerland_2002]. Creating a control sample alongside the original
experimental design is recommended; models are often fit to control samples
before finding ChIP enriched regions [@Park_2009]. Statistical techniques such
as LOWESS regression have been used to standardize samples successfully, albeit
with limitations in consequent downstream analyses [@Taslim_2009]. Bootstrap
sampling has been introduced to maximize reproducibility between samples
[@Kallio_2013]. Most existing analytical methods have a specialized nature,
which is in direct conflict with the nature of biological signal detection.
Approaches adopted from the signal processing domain have been ported to
deep-sequencing data [@Kumar_2013]. 

Once enriched regions have been obtained, detected regions are evaluated in
terms oftheir overlap with high scoring sequence motifs obtained by
biologically oriented experiments. ChIP-seq of proteins with diffuse binding
patterns, such as epigenetic histone modifications that are distributed over
larger genomic regions remain largely unexplored. Algorithms with wide genomic
ranges and increased tag densities coupled with alternative encodings of
binding preferences might be a successful approach [@Szalkowski_2010].
Prospective downstream analyses must be considered before deciding on a given
method. 


## Finding enrichment in ChIP-Seq data: Peak calling

The traditional approach in ChIP-Seq analyses for the identification of
markedly enriched read densities in ChIP samples, or "peaks", is named peak
calling [@Bardet_2011]. The goal of said process is to filter out background
noise and accurately identify the locations of peaks in the live genome. Said
peaks usually correspond to Transcription Factor Binding Sites,
but this is not always the case; nonspecific, indirect DNA binding proteins may
be associated to peak sequences [@Valouev_2008]. The shape of peaks varies
among proteins. However, it's possible to characterize them in three general
categories, each with its corresponding biological associations. "sharp" peaks
corresponding to specific genomic positions. "broad" peaks which are associated
with large genomic domains and "mixed" peaks, involving a combination of the
two aforementioned peak types. Histone modification ChIP-Seq experiments are
usually associated with broad peaks [@Nakato_2016]. In certain cases,
integration of both sharp and broad calling methods may be useful for the
proper characterization of underlying biological phenomena
[@Young_2011][@Starmer_2016]. However, in the particular case of histone marks,
known binding sites are mostly nonexistent. Nonetheless, BCP and MUSIC have
shown to be the best performing algorithms for peak calling from histone data.
Once peaks are found, they must be tested for statistical significance and
finally, enrichment analysis must be performed. Poisson tests have been shown
to have greater predictive power than Binomial tests [@Thomas_2016].

It is important to mention that this analysis step isn't exempt from biological
artefacts. It has been observed that active promoters give rise to false
positive 'Phantom Peaks' in ChIp-seq experiments [@Jain_2015]. Most peak
calling methods can be placed into the following three categories: Window based, 
overlap based and Probabilistc model based methods [@Laajala_2009].

Window based methods count the number of reads mapped within a "region" or two
points in the genome. MACS2 and MUSIC are among the most representative
[@Zhang_2008]. MACS2 empirically models the shift size of ChIP-Seq tags and
uses them to improve the spatial resolution of predicted binding sites.
Afterwards, it tests peaks with a poisson distribution model. MUSIC utilizes a
signal processing approach to filter out systematic noise. It performs
multiscale decomposition using median filtering, identifying enriched regions
at multiple length scales in the process [@Harmanci_2014]. 

While choosing proper window parameters is a lengthy empirical process that can
yield variable results, it's been shown that methods that incorporate variable
window sizes generally perform better than methods that do not [@Thomas_2016].

Overlap based approaches identify local maxima where reads overlap with each
other and set the start and end of a genomic region accordingly. FindPeaks is a
notable example [@Fejes_2008]. This type of approach relies on the premise that
regions in the genome where reads cluster into "regions of enrichment"
represent in vivo locations with proteins of interest. A minimum height
threshold is used to determine which peaks are shown. Afterwards, it collects
and sorts reads along provided chromosomes and provides any found areas of
enrichment.

Probabilistic model based peak callers describe read agglomeration in the
genome as a sequence of states binding sites and background. Hpeak is a notable
example for this approach. Based on a hidden markov model (HMM), it allows for rigorous
statistical inference with the added bonus of ease of interpretation that
dealing in probabilities provides; i.e. peak comparisons [@Qin_2010]. It
assumes realistic probability distributions from data and incorporates a
model-specific weighting scheme on sequencing read coverage. It is also
possible to incorporate other information, such as GC content and genomic
region features. Hpeak's True positive rates are among the highest in
competing models. GEM, another highly utilized model, links binding event and
motif discovery in the context of a generative probabilistic model according to
the positional priors based on a given genome sequence [@Guo_2012].

It's important to note that if adequate energy models are available, using a
probabilistic based approach provides no discernible advantage and can
introduce error into sample analysis, due to probabilistic models assuming
non-independence of binding affinity when biologically this is not the case
[@Ruan_2017]

## Joint Analysis

When multiple ChIP-seq data sets are available, integrating them is not a
trivial task. Peak callers are not able to integrate between-sample
information, making overall results dependant on per sample peak calling
performance. Joint sample analysis solves said problem. Algorithms that are
commonly used in joint methods include Expectation Maximization, Maximum
Likelihood estimators and Markov Random Field Models.

MultiGPS is a model based on a generalized Expectation Maximization algorithm
that allows it to share information across samples and experiments while
replicating specific noise sources and providing consistent binding locations
[@Mahony_2014]. 

jMOSAiCS utilizes a joint, binomial distribution based, custom probability
layered model that estimates enrichment from read counts. It is able to capture
all possible enrichment patterns for a large number of histone mark datasets,
since the joint distribution of enrichment variables is governed by a
univariate variable [@Zeng_2013]. Other layered models, such as SignalSpider,
which assumes a Gaussian distribution, have shown comparable success
[@Wong_2014].

Markov random fields have proven to be successful approaches. Bao et al's model
accounts for spatial dependencies in the data, by assuming first-order Markov
dependence and zero inflated mixture distributions to account for the large
proportion of zero count observations. It also permits joint modeling of
multiple experiments by incorporating key aspects involved in the experimental
design of the ChIP data used [@Bao_2013].

Recently, integrative methods have been developed to segment, classify and
annotate a whole-genome sequence de novo, based on unsupervised
machine-learning methods. These methods directly receive all ChIP sample data
and analyze it simultaneously, instead of calling peaks and comparing them
individually. Joint Analysis methods may be augmented by said data segmentation
techniques. A typical segmentation procedure involves finding boundaries of
interest within the genome and simultaneously assigning labels relevant to the
model.

Several models have been proposed for unsupervised use with histone mark
scenarios. Based on a hidden Markov model, chromHMM partitions a reference
genome into multiple chromatin states based on histone modification datasets.
It partitions provided aligned reads or peak calls and estimates enrichment
patterns via a Poisson background distribution, from which it can learn
chromatin states and characterize their biological functions [@Ernst_2012].
Comparably, Hoffman et al. implemented Segway, a dynamic bayesian network
[@Hoffman_2012]. While similar in the underlying probabilistic model, the main
difference between both models is one of resolution. Segway is able to operate
on the full dataset provided, which allows for finer segmentation. This allows
for the detection of elements at a resolution lower than that of nucleosomes
and distinct advantages when handling missing data. Both approaches can
successfully reduce otherwise highly heterogenous data into biologically-clear,
interpretable patterns. 

Empirical Bayes approaches have been successfully implemented for the modelling
of histone modification data. Starting from separate, per-cell line single
analysis, Ferguson, et al's model utilizes an EB algorithm for joining said
analyses implicitly [@Ferguson_2012].

# Related Work

Learning models aim to provide lower dimensional views for high dimensional
data. Said features are refined during the learning process. It is possible to
capture and summarize various layers of information within cells. In the case
of TF binding, sequence motifs are the usual representation for extracting said
knowledge [@Park_2015]. 

To train a supervised model, "Bound" or "Unbound" labeled target sequences
derived from ChIP-Seq data must be provided. Usual models include support
vector machines, logistic regression, random forests or recently, deep-learning
based approaches [@Quang_2017].

Methods regarding the state of histone modifications generally take an
unsupervised approach due to the low availability of True positive and negative
TFBS data available. Predictions may then be clustered in a manner similar to
supervised methods, into "Bound" or "Unbound". Other methods may use
statistical tests or binding affinity prediction scores [@Keilwagen_2017].

Keilwagen et al's method, models the joint distribution of a wide array of
numeric features by discrete distributions. Their learning principle consists
of using a weighted variant of the maximum conditional likelihood principle.
Thorough feature selection and engineering is performed from diverse deep
sequencing protocols. Novel statistics are then computed from said features,
utilized for appropriate data segmentation and binning, and wrapped into a
supervised machine learning algorithm related to logistic regression. It
finally combines the trained classifier predictions obtained from training on
different cell types and iterations in an ensemble approach [@Keilwagen_2017]. 

Deep neural networks have seen widespread adoption for a myriad of prediction
related tasks, notably, in answering the protein and other sequence interacting
structure function prediction problem. Specifically, transcription factor
binding affinity. Deepbind [@Alipanahi_2015], a trendsetter, incorporates a
convolutional neural network in two steps. First, it applies a convolution
module to learn motifs in a PWM representation. Afterwards, it utilizes a
nonlinear neural network to combinatorially fabricate prediction motifs. Said
model consistently outperformed all known models at the time. A similar effort
was employed by DeepSEA, which utilized a mixture of convolutional, pooling and
fully connected layers[@Zhou_2015]. 

Similar deep architectures have been utilized in the case of models such as
Deep NF [@Gligorijevi__2017], a Multimodal Deep Autoencoder based model.
Through a combination of STRING networks, it constructs common low-dimensional
representations containing high-level protein features. Whole regulatory
networks are recreated and then predictions are made from said constructed
networks.Evaluation metrics for this study contain cross-validation and
temporal holdout on the predictive performance. 

Cell specificity studies have been successful with Deep Learning approaches.
Such is the case with TFImpute, whose main objective is data imputation on
incomplete datasets, as is often the case with TF specificity information
[@Qin_2017]. They model the TF-Binding problem in a multi-task learning setting
that borrows information across transcription factors and cell lines; it is also
capable of predicting cell line specific enhancer activities. FactorNet
[@Quang_2017] works in a similar fashion.

Learning models from alternative paradigms have been cross-implemented
successfully. BindSpace
^[https://www.biorxiv.org/content/early/2018/06/29/359539] learns to embed DNA
sequences based on TF Class/family labels. They applied
StarSpace^[https://arxiv.org/abs/1709.03856],  a recent Natural Language
Processing (NLP) algorithm to the previously mentioned feature space. Said
model learns an embedding of words into a semantic space; in this case, k-mers
to DNA probe sequences. It then maximizes the similarity between a sequence
example and its corresponding layers.

Transfer learning approaches modeled under Bayesian Frameworks may aid in
modeling the predictive relationship between transcription factors and Gene
Expression in cases of high sequence degeneracy [@Zou_2015]. It's basic idea is
to reuse old domain samples/instances as auxiliary data for a new domain (i.e.
general to specific histone datasets). Zou et al's model presents a parameter
transfer model for degenerate systems, which may be useful in the case for
certain instances of cancer.


## Benchmarking and associated metrics 

There is no currently agreed upon absolute benchmarking metric. This is due in
part to scarcely available information regarding true positive and true
negative TFBS; the evaluation of peaks as putative TFBS remains a
biologically-oriented trial and error process. Current approaches often compare
against the few known binding sites, low-throughput experiments such as PCR,
simulation studies, or recently, supervised learning methods emerging from
manual annotation [@Hocking_2016]. Additional studies have reported that
although sensitivity and specificity have reached a common threshold between
peak caller performance on the same dataset, position accuracy of TFBS remains
diverse. Greater variability can be seen when using peak callers for biological
experiments with designs aimed to unravel different phenomena (histone
modifications or point TF binding) [@Wilbanks_2010]. Integrating phenomenon
specific information, such as data from histone marks has been shown to enhance
predictions [@Pique_Regi_2010]. 

Not all metrics are useful for elucidating biological mechanisms. The
appropriate method depends on the species, sample conditions and target
proteins. It is suggested that incorporating both the algorithmic
characteristics that yield the best predictive results and the most explicative
features given the chosen biological phenomenon that is trying to be modeled
will yield the best results [@Weirauch_2013].

Reproducibility of findings may be assessed with the Irreproducible discovery
rate (IDR) metric. It assesses the rank consistency of common peaks between two
replicates. It is possible to assess each peak independently. It's considered a
robust metric, since no arbitrary thresholds are considered and input signals
don't require calibration before utilization. Therefore, all noteworthy peaks
and regions are included in the evaluation [@Li_2011].

A general, widely utilized metric for calculating prediction success after
enrichment analyses is the Receiver operating characteristic (ROC) curve and
it's corresponding Area Under Curve (AUC) calculation. This metric is
determined by plotting the obtained true positive rate against the false
positive rate [@Mathelier_2013].

The ENCODE-DREAM Challenge
^[http://dreamchallenges.org/project/encode-dream-in-vivo-transcription-factor-binding-site-prediction-challenge/]
has received notable attention and a number of state-of-the-art models have
emerged from it. Said challenge aims to assess the performance of tools for
predicting cell type specific TF binding using minimal experimental data. The
origins of the data provided is only known to the contest's organizing
committee, which calls for generalizable models. Said contest seeks to obtain
the best performing model irrespective of previous data processing, selection
of negative background and evaluation measures under one sole scoring metric.

## Machine Learning in Computational Biology

In summary, Machine Learning methods are applied in Computational Biology as general-purpose approaches to learn functional relationships from data; this allows for the deriving of predictive models without the need of previous assumptions about underlying biological mechanisms [@Angermueller_2016]. A typical workflow can be observed in Figure 3. 

![Typical Machine Learning workflow. Obtained from [@Angermueller_2016].](./Figures/mlwf.jpg)

Two machine learning paradigms have been widely adopted for application in the biological domain: Supervised and Unsupervised learning. Briefly, the former aims to predict the value of an outcome measure based on a number of input measures; the latter aims to discover patterns and associations from data samples without a strict need for output labels [@Hastie_2009]. Table 1 shows a brief recopilation of recently implemented models for the prediction of Transcription Factor Binding Sites from histone data, their learning paradigms and their underlying prediction models. 

Table: Recent machine learning models to predict Transcription Factor Binding Sites from histone data.

| Name                 | Learning Paradigm | Model                      | Reference          |
|----------------------+-------------------+----------------------------+--------------------|
| CENTIPEDE            | Unsupervised      | Hierarchical Mixture Model | [@Pique_Regi_2010] |
| Arvey et al. (2012)  | Supervised        | SVM                        | [@Arvey_2012]      |
| Gusmao et al. (2014) | Unsupervised      | Hidden Markov Model        | [@Gusmao_2014]     |
| Romulus              | Unsupervised      | Expectation Maximization   | [@Jankowski_2016]  |
| FactorNet            | Deep Supervised   | CNN                        | [@Quang_2017]      |
| TFImpute             | Deep Supervised   | DNN                        | [@Qin_2017]        |

\newpage
# Objectives

## General

Design and implement algorithms to predict H3K27ac-specific Transcription Factor Binding Sites
from ChIP-Seq and other NGS related data.

## Specific

1. Establish feature selection criteria
2. Extract relevant information for TFBS prediction
3. Select data representation that preserves the most prediction-relevant
   information
4. Decide on a given assesment criterion to evaluate the chosen model
5. Determine most effective model architectures from current state-of-the-art
   models.
6. Select model with highest accuracy for H3K27ac TFBS
7. Develop an R package for ease of model use and access

## Research Questions

* Which of the currently available state-of-the-art algorithms can successfully predict a higher
  number of TFBS in datasets where the H3K27ac modification is present?
* Which of their features have the most TFBS predictive power?
* Are all data representations equal? Can data be represented in a way that
  information relevant to prediction is optimally conserved?
* What are the predictive limitations of ChIP-Seq data? How can they be
  addressed in the model's implementation?
* How do we assess the biological relevance of our results?

## Relevance of Research

While a myriad of models and their associated software tools are readily
available for the prediction of TFBS, context-specific predictions are often
poor. Computationally, H3K27ac specific datasets have a set of biologically
associated biases that must be accounted for when extracting features relevant
to the prediction task; this is a feature extraction problem. Optimal data
representations that will provide the most information to H3K27ac TFBS
predicting models are yet to be found and established.  Developing models that
can integrate said representations and successfully predict TFBS remains a
phenomena specific task; this is a classification problem.  Models that can
predict chromatin-state specific TFBS binding are being actively developed;
none of them report H3K27ac specific features being taken into account when
constructing a model.

Biologically, H3K27ac has been associated to cancer. Providing researchers with
an accessible method to generate predictions to find precise, novel molecular
targets to further advance research will prove to be effective at reducing the
heavy resource expenditure commonly found in biological research. 

## Limitations and Key Assumptions

We make the following assumptions throughout the development of this research
project:

* Building a model that integrates features extracted from publicly available datasets
will yield a model with increased precision as opposed to a model that's
built with a general TFBS prediction paradigm.

* ChIP-Seq data has potential representations that are relevant to predicting
  TFBS in H3K27ac.

* Non-Algorithmic bias sources, specifically Biological experiment related, will have been accounted
  for previously by the laboratory that made it public. 

* Our model will employ a variety of normalization techniques. We assume this
  will aid in the process of discerning true features from statistical or biological artefacts.

The following potential limitations have been identified in the development of
this research project:

* **Availability of datasets** : We depend on the international scientific
community to provide open access datasets from biological experiments that are relevant
to our prediction problem. The quality and H3K27ac-specificty of said datasets
will likely have a correlation with our model's overall predicting quality.

* **Lack of Annotation** : "True" TFBS locations are not thoroughly
characterized. This reduces our "True" training observations. Said shortcoming
will have to be circumvented via efficient data segmentation or unsupervised
learning paradigms.

* **Computing power** : Deep Learning methods that have proven to be among the
best at the prediction task require specialized, often hefty, computational resources. Depending
on our model's architecture, this may or may not rend the training of our model
unfeasible. 

* **Biological Relevance** : While our model may yield high prediction
accuracy, it is not possible to verify it's usefulness until a corresponding
biological experiment has been developed and evaluated with our model. Cell and
Tissue type biases are bound to play a role in the effectiveness of our
prediction model.

\newpage

# Methodology

This research project has been segmented into the following phases to allow for
successful completion of the aforementioned research objectives.

* **Step 1 |  Data Exploration and Characterization** : Publicly available NGS
data is abundant. However, not all of it is relevant to the H3K27ac TFBS
prediction problem. To determine which datasets will allow us to build a
successful prediction model, a thorough literature review must be made.
Consequently, datasets that have been reported to be useful in constructing
models will be evaluated for their relevance to H3K27ac-specific predictions.
Parameters to search for include but are not limited to: Precise annotation,
biological condition association, chromatin state specificity and ease of
access.

* **Step 2 | Benchmarking of Current Methods** : An exhaustive literature and
algorithm search will be conducted to determine which models have been
previously successful at answering the TFBS prediction problem. These will be
reimplemented and evaluated on the previously chosen datasets. Those that yield
the most precise results for known targets in H3K27ac will be kept for further
model development.

* **Step 3 | Opportunity Assessment** : Previously evaluated models will most
certainly have model-specific shortcomings. Additionally, They will lack H3K27ac
context specificity. Models will be assessed based on baseline performance;
those with the highest will be kept for further tuning.

* **Step 4 | Model Development** : Once a model or a combination of them are
chosen, H3K27ac specific features will be integrated into the model. This will
ideally result in a thorough, context specific model.

* **Step 5 | Product Development** : A working implementation of the previously
constructed model will be elaborated in the form of an R package for ease of
access and use. If said model proves to be resource intensive, proper pipelines
for handling the scale of said model will be developed.

* **Step 6 | Wrap-up** : Results will be analyzed and inferences will be drawn
based on algorithmic performance. Said results will be contrasted to current
state-of-the-art models and areas of opportunity will be identified.

The development of the model will be conducted in the R language for
statistical computing with the possibility of incorporating diverse packages
from its Bioconductor package manager; the package that is to be developed will
adhere to the aforementioned manager's standards. When required, it's Rcpp
extension for incorporating C++/C elements will be utilized. Python scripts may
be used in a production setting. Since many genomics oriented tools are
developed for the command line, a Linux based operating system will be used
throughout the process.
